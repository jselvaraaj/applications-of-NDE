pygame 2.3.0 (SDL 2.24.2, Python 3.9.15)
Hello from the pygame community. https://www.pygame.org/contribute.html
State dim:  152
Generating data...
Generating data for policy 1...
Generating data for policy 2...
Generating data for policy 3...
Generating data for policy 4...
Generating data for policy 5...



Baseline policy
Training model...
Epoch: 0 Training loss: 0.2147434949874878
Epoch: 1 Training loss: 0.21358981728553772
Epoch: 2 Training loss: 0.21241173148155212
Epoch: 3 Training loss: 0.21120595932006836
Epoch: 4 Training loss: 0.20996935665607452
Epoch: 5 Training loss: 0.20869867503643036
Epoch: 6 Training loss: 0.20739036798477173
Epoch: 7 Training loss: 0.20604078471660614
Epoch: 8 Training loss: 0.20464588701725006
Epoch: 9 Training loss: 0.20320144295692444
Epoch: 10 Training loss: 0.20170292258262634
Epoch: 11 Training loss: 0.2001453936100006
Epoch: 12 Training loss: 0.1985236406326294
Epoch: 13 Training loss: 0.19683212041854858
Epoch: 14 Training loss: 0.19506491720676422
Epoch: 15 Training loss: 0.19321604073047638
Epoch: 16 Training loss: 0.1912790685892105
Epoch: 17 Training loss: 0.18924759328365326
Epoch: 18 Training loss: 0.18711507320404053
Epoch: 19 Training loss: 0.18487530946731567
Epoch: 20 Training loss: 0.1825227290391922
Epoch: 21 Training loss: 0.18005305528640747
Epoch: 22 Training loss: 0.17746388912200928
Epoch: 23 Training loss: 0.1747555285692215
Epoch: 24 Training loss: 0.1719316989183426
Epoch: 25 Training loss: 0.16900095343589783
Epoch: 26 Training loss: 0.1659785807132721
Epoch: 27 Training loss: 0.1628895252943039
Epoch: 28 Training loss: 0.15977226197719574
Epoch: 29 Training loss: 0.1566821038722992
Epoch: 30 Training loss: 0.15369148552417755
Epoch: 31 Training loss: 0.15088088810443878
Epoch: 32 Training loss: 0.14830538630485535
Epoch: 33 Training loss: 0.14592646062374115
Epoch: 34 Training loss: 0.1435752958059311
Epoch: 35 Training loss: 0.14107687771320343
Epoch: 36 Training loss: 0.1384296864271164
Epoch: 37 Training loss: 0.13577759265899658
Epoch: 38 Training loss: 0.13323871791362762
Epoch: 39 Training loss: 0.13081370294094086
Epoch: 40 Training loss: 0.1284235119819641
Epoch: 41 Training loss: 0.12599465250968933
Epoch: 42 Training loss: 0.12351127713918686
Epoch: 43 Training loss: 0.12101338803768158
Epoch: 44 Training loss: 0.11855578422546387
Epoch: 45 Training loss: 0.11615507304668427
Epoch: 46 Training loss: 0.11377459764480591
Epoch: 47 Training loss: 0.11137769371271133
Epoch: 48 Training loss: 0.10897722095251083
Epoch: 49 Training loss: 0.10660506784915924
Epoch: 50 Training loss: 0.10426165908575058
Epoch: 51 Training loss: 0.10192443430423737
Epoch: 52 Training loss: 0.09958840906620026
Epoch: 53 Training loss: 0.09727559238672256
Epoch: 54 Training loss: 0.09500870853662491
Epoch: 55 Training loss: 0.09279362857341766
Epoch: 56 Training loss: 0.09063258767127991
Epoch: 57 Training loss: 0.08853882551193237
Epoch: 58 Training loss: 0.08652769029140472
Epoch: 59 Training loss: 0.08460642397403717
Epoch: 60 Training loss: 0.08278011530637741
Epoch: 61 Training loss: 0.08105912804603577
Epoch: 62 Training loss: 0.07945553958415985
Epoch: 63 Training loss: 0.07797684520483017
Epoch: 64 Training loss: 0.07662638276815414
Epoch: 65 Training loss: 0.07540600746870041
Epoch: 66 Training loss: 0.07431568205356598
Epoch: 67 Training loss: 0.07335110753774643
Epoch: 68 Training loss: 0.07250258326530457
Epoch: 69 Training loss: 0.07175865769386292
Epoch: 70 Training loss: 0.07111331075429916
Epoch: 71 Training loss: 0.07056631147861481
Epoch: 72 Training loss: 0.07011403888463974
Epoch: 73 Training loss: 0.06974335759878159
Epoch: 74 Training loss: 0.06943409889936447
Epoch: 75 Training loss: 0.06916967034339905
Epoch: 76 Training loss: 0.06894583255052567
Epoch: 77 Training loss: 0.06876084208488464
Epoch: 78 Training loss: 0.0686035081744194
Epoch: 79 Training loss: 0.06846321374177933
Epoch: 80 Training loss: 0.06833317875862122
Epoch: 81 Training loss: 0.06820741295814514
Epoch: 82 Training loss: 0.06807968020439148
Epoch: 83 Training loss: 0.06794273108243942
Epoch: 84 Training loss: 0.06778762489557266
Epoch: 85 Training loss: 0.067605160176754
Epoch: 86 Training loss: 0.06739579886198044
Epoch: 87 Training loss: 0.06720274686813354
Epoch: 88 Training loss: 0.06714901328086853
Epoch: 89 Training loss: 0.06741044670343399
Epoch: 90 Training loss: 0.06721901893615723
Epoch: 91 Training loss: 0.067254938185215
Epoch: 92 Training loss: 0.06698145717382431
Epoch: 93 Training loss: 0.06698662042617798
Epoch: 94 Training loss: 0.06701011955738068
Epoch: 95 Training loss: 0.06709340959787369
Epoch: 96 Training loss: 0.06710183620452881
Epoch: 97 Training loss: 0.06698984652757645
Epoch: 98 Training loss: 0.0669054165482521
Epoch: 99 Training loss: 0.06687051802873611
Testing model...
Test loss: 0.07844983786344528



Test policy
Training model...
Started making the X data continuous
Finished making the X data continuous
Epoch: 0 Training loss: 0.3041135370731354
Epoch: 1 Training loss: 0.24647504091262817
Epoch: 2 Training loss: 0.22596777975559235
Epoch: 3 Training loss: 0.21690721809864044
Epoch: 4 Training loss: 0.21016047894954681
Epoch: 5 Training loss: 0.20566882193088531
Epoch: 6 Training loss: 0.20266810059547424
Epoch: 7 Training loss: 0.20062905550003052
Epoch: 8 Training loss: 0.1990847885608673
Epoch: 9 Training loss: 0.1976049840450287
Epoch: 10 Training loss: 0.1959742307662964
Epoch: 11 Training loss: 0.19417135417461395
Epoch: 12 Training loss: 0.19231350719928741
Epoch: 13 Training loss: 0.19046705961227417
Epoch: 14 Training loss: 0.18873031437397003
Epoch: 15 Training loss: 0.1870938092470169
Epoch: 16 Training loss: 0.18550622463226318
Epoch: 17 Training loss: 0.18394628167152405
Epoch: 18 Training loss: 0.18238915503025055
Epoch: 19 Training loss: 0.18085210025310516
Epoch: 20 Training loss: 0.1793075054883957
Epoch: 21 Training loss: 0.17777442932128906
Epoch: 22 Training loss: 0.1762048602104187
Epoch: 23 Training loss: 0.17463791370391846
Epoch: 24 Training loss: 0.1730470210313797
Epoch: 25 Training loss: 0.17142713069915771
Epoch: 26 Training loss: 0.16977056860923767
Epoch: 27 Training loss: 0.16809657216072083
Epoch: 28 Training loss: 0.16638781130313873
Epoch: 29 Training loss: 0.16465245187282562
Epoch: 30 Training loss: 0.16289114952087402
Epoch: 31 Training loss: 0.16108304262161255
Epoch: 32 Training loss: 0.15923736989498138
Epoch: 33 Training loss: 0.15735851228237152
Epoch: 34 Training loss: 0.15544256567955017
Epoch: 35 Training loss: 0.15347763895988464
Epoch: 36 Training loss: 0.1514834314584732
Epoch: 37 Training loss: 0.14942944049835205
Epoch: 38 Training loss: 0.14734628796577454
Epoch: 39 Training loss: 0.1452188342809677
Epoch: 40 Training loss: 0.14303450286388397
Epoch: 41 Training loss: 0.1408323049545288
Epoch: 42 Training loss: 0.13858358561992645
Epoch: 43 Training loss: 0.13628853857517242
Epoch: 44 Training loss: 0.1339583396911621
Epoch: 45 Training loss: 0.13159506022930145
Epoch: 46 Training loss: 0.12919586896896362
Epoch: 47 Training loss: 0.12677021324634552
Epoch: 48 Training loss: 0.12432543933391571
Epoch: 49 Training loss: 0.12184042483568192
Epoch: 50 Training loss: 0.1193472221493721
Epoch: 51 Training loss: 0.11683616042137146
Epoch: 52 Training loss: 0.11430932581424713
Epoch: 53 Training loss: 0.11178233474493027
Epoch: 54 Training loss: 0.10924722999334335
Epoch: 55 Training loss: 0.10670128464698792
Epoch: 56 Training loss: 0.10417410731315613
Epoch: 57 Training loss: 0.10165941715240479
Epoch: 82 Training loss: 0.057516638189554214
Epoch: 83 Training loss: 0.05653681233525276
Epoch: 84 Training loss: 0.05559837073087692
Epoch: 85 Training loss: 0.05470306798815727
Epoch: 86 Training loss: 0.05385090038180351
Epoch: 87 Training loss: 0.0530446395277977
Epoch: 88 Training loss: 0.0522477924823761
Epoch: 89 Training loss: 0.051488809287548065
Epoch: 90 Training loss: 0.05076606944203377
Epoch: 91 Training loss: 0.05007486417889595
Epoch: 92 Training loss: 0.04941781610250473
Epoch: 93 Training loss: 0.04878693446516991
Epoch: 94 Training loss: 0.048166871070861816
Epoch: 95 Training loss: 0.04758114740252495
Epoch: 96 Training loss: 0.04701903462409973
Epoch: 97 Training loss: 0.04647737368941307
Epoch: 98 Training loss: 0.045942094177007675
Epoch: 99 Training loss: 0.04542844370007515
Testing model...
Test loss: 0.05448063462972641